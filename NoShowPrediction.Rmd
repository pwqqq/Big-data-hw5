---
title: "prediction"
author: "Annie Yang"
date: "5/7/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This project aims to predict and improve the missed opportunity rate (no-show rate) for medical centers. The data is stored in the dataset Predict_NoShow_Train.csv with 180,000 observations and 16 predictors.

```{r}
# import libraries
library(xgboost)
library(data.table)
library(Matrix)
library(pROC)
library(dplyr)
library(purrr)
library(tidyverse)
library(lubridate)
library(Matrix)
library(MLmetrics)
library(caret)
library(glmnet)
```

First, we imported the original dataset and cleaned the missing data. Since no missing value was found, all the 180,000 observations are included in the following analysis and prediction. 
```{r}
# Import data
train <- fread("Predict_NoShow_Train.csv")

# Check missing data

train%>%
  map_df(function(col) sum(is.na(col)))%>%
  gather()%>%
  summarise(count_of_nas = sum(value,na.rm=T))

### there is no missing data in the dataset
```

Then, some variables are transformed or removed from the training set for better prediction. We extracted the Month and Year from the variable Date of Appointment and made Month as separate predictive variables. When making analysis, we chose to focus more on the data of appointment because the date of appointment could be correlated with the missing opportunity rate. People could chose to not show-up due to the weather or time factors. 
```{r}
# Manipulate date
#train$DateAppointmentWasMade <- as.Date(train$DateAppointmentWasMade)
train$DateOfAppointment <- as.Date(train$DateOfAppointment)
# extract month and year information
train$month <- format(train$DateOfAppointment,"%m")
train$year <- format(train$DateOfAppointment,"%y")
```

To make further and more complete analysis, we made barplots to interpret the ratio of No Show-up rate in different months and years. The ratio of No-Show rate varies across the year, from the lowerst 0.282 in January to highest 0.32 in December. Because of the fluctuation, month is considered as a predictive variable in the model.

```{r}
# appointment data
# frequency distribution for month
month_s <- train$month[which(train$Status == "Show-Up")]
month_ns <- train$month[which(train$Status == "No-Show")]

month_s_freq <- data.frame(table(month_s))
month_ns_freq <- data.frame(table(month_ns))


Month <- c("01", "02","03","04","05","06","07","08","09","10","11","12")
month_s_freq <- c(month_s_freq$Freq)
month_ns_freq <- c(month_ns_freq$Freq)
text <- month_ns_freq/(month_s_freq+month_ns_freq)
data <- data.frame(Month, month_s_freq, month_ns_freq,text)

p1 <- plot_ly(data, x = ~Month, y = ~month_s_freq, type = 'bar', name = 'Show-Up',text=text) %>%
  add_trace(y = ~month_ns_freq, name = 'No-Show') %>%
  layout(yaxis = list(title = 'Count'), barmode = 'stack')
p1
```

Then we chose to visualize the rate of No-Show rate for year as well. From the below graph, we can tell that the No-Show rate for year 2014 and 2015 are almost the same at the level 0.30, which means that different year would not affect the No-Show rate. So, year variable is excluded from the predictive model.
```{r}
# appointment data
# frequency distribution for year
year_s <- train$year[which(train$Status == "Show-Up")]
year_ns <- train$year[which(train$Status == "No-Show")]

year_s_freq <- data.frame(table(year_s))
year_ns_freq <- data.frame(table(year_ns))

Year <- c("2014","2015")
year_s_freq <- c(year_s_freq$Freq)
year_ns_freq <- c(year_ns_freq$Freq)
text <- year_ns_freq/(year_s_freq+year_ns_freq)
data <- data.frame(Year, year_s_freq, year_ns_freq,text)

p2<- plot_ly(data, x = ~Year, y = ~year_s_freq, type = 'bar', name = 'Show-Up',text=text) %>%
  add_trace(y = ~year_ns_freq, name = 'No-Show') %>%
  layout(yaxis = list(title = 'Count'), barmode = 'stack')
p2
```

The data after cleaning has 180,000 observations and 14 variables and will be used as the training data later. The result No-Show is set to be 1, and Show-up as 0. Then we fit logistic regression, Penalized logistic regression, and XGBoost to train the dataset and make comparisons with the test data.

```{r}
# Remove unnecessary variable
data.train <- subset(train, select = -c(ID,DateAppointmentWasMade,DateOfAppointment,year))
```

The first method we tried is logistic regression.After factorization, the result is 1 when the person did not show and is 0 when the person showed up, which is a binary result. So we chose to fit a logistic regression, which could be used to model the probability of not showing up. In this model, month is factorized, and a logistic regression is fit on the 14 variables.

```{r}

data.train$Gender_n = ifelse(data.train$Gender=='M',1,0)
data.train$status = ifelse(data.train$Status=='No-Show',1,0)
day_week = model.matrix(~DayOfTheWeek-1, data.train)

train = data.train %>%
  select(-Gender, -Status, -DayOfTheWeek)

train_df = cbind(train,day_week)

train_numeric = data.frame(apply(train_df,2,as.numeric))

train_numeric$month= as.factor(train_numeric$month)

mod_logi = glm(status~., data = train_numeric, family = 'binomial')
summary(mod_logi)

pred.glm = predict(mod_logi,train_numeric[,-13],type = 'response')
## the error massage shows: prediction from a rank-deficient fit may be misleading
## which means the predictors might have problem of colinearity

```

In order to deal with the colinearity, we conduct a PDA with mixed data, since the dataset contains continuous and categorical variables. 

```{r}
## load train dataset and two test datasets
library(readr)
Predict_NoShow_Train <- read_csv("Predict_NoShow_Train.csv")
pred_pr = read_csv("Predict_NoShow_PrivateTest_WithoutLabels.csv")
pred_pub = read_csv('Predict_NoShow_PublicTest_WithoutLabels.csv')

## check the dimensions
dim(Predict_NoShow_Train)
dim(pred_pr)
dim(pred_pub)

## add status column to 2 test datasets
pred_pr$Status = 'No-Show'
pred_pub$Status = 'No-Show'

## combine 3 datasets and do PCA
all_data = rbind(Predict_NoShow_Train, pred_pr, pred_pub)

all_data$Gender_n = ifelse(all_data$Gender=='M',1,0)

all_data$DateAppointmentWasMade = as.Date(all_data$DateAppointmentWasMade,'%Y-%m-%d')
all_data$DateOfAppointment = as.Date(all_data$DateOfAppointment,'%Y-%m-%d')

all_data$month_of_app = as.numeric(format(all_data$DateOfAppointment, "%m"))
all_data$status = ifelse(all_data$Status  == 'Show-Up',0,1)

day_week = model.matrix(~DayOfTheWeek-1, all_data)

## delete redundant variables
library(dplyr)
Xs =all_data %>%
  select(-Status) %>%
  select(-ID) %>%
  select(-DateAppointmentWasMade) %>%
  select(-DateOfAppointment) %>%
  select(-Gender) %>%
  select(-DayOfTheWeek)

Xss = cbind(Xs,day_week)

Xss_mix = Xss %>%
  select(-status) %>%
  data.frame()

# to factor
Xss_mixs = data.frame(cbind(apply(Xss_mix[,1:2],2,as.numeric),apply(Xss_mix[,-1:-2],2,as.factor)))

# change first 2 variable to numeric
Xss_mixs$Age = as.numeric(Xss_mixs$Age)
Xss_mixs$DaysUntilAppointment = as.numeric(Xss_mixs$DaysUntilAppointment)

y_train = Xss$status[1:nrow(Predict_NoShow_Train)]

install.packages('PCAmixdata')
library(PCAmixdata)

X.quanti = splitmix(Xss_mixs)$X.quanti
X.quali = splitmix(Xss_mixs)$X.quali

## PCA analysis with mixed data and select 30 dimensions, which explain 96.60317% variance
X_mf = PCAmix(X.quanti, X.quali,ndim = 30,rename.level=TRUE,graph = F)

# the eigenvalue and variance explained
X_mf$eig

# the orthogonal matrix 
X_pca = X_mf$ind$coord
dim(X_pca)
```

It is worth noting that after the PDA, we cannot use the orthogonal matrix to fit generalized linear model, since the generalized linear model. On the other hand, machine learning methods like neural network is friendly to the orthogonal data and we train a neural network in the following part.

```{r}
## convert to train data(train data after PCA) the test data(2 test data set after PCA) into data.matrix
train_matrix = data.matrix(X_pca)[1:nrow(Predict_NoShow_Train),]
pred_matrix = data.matrix(X_pca)[-1:-nrow(Predict_NoShow_Train),]

#select a sample from train_matrix
data_set_size1 = nrow(train_matrix) * .7
set.seed(1234)
indexes1 = sample(1:nrow(train_matrix), size = data_set_size1)

train.x = train_matrix[indexes1,]
test.x = train_matrix[-indexes1,]

# combine predictors and outcome
train_df = data.frame(cbind(train.x,y_train[indexes1]))
train_df$V31 = make.names(as.factor(train_df$V31))

# rename the variables
names = make.names(paste('dim',seq(1,30,length=30)))
colnames(train_df) = c(names,'y')

# tune the parameters (takes almost 2 hours to run)
library(caret)
## 10 folder cross-validation
numFolds = trainControl(method = 'cv', number = 10, classProbs = T, verboseIter = TRUE)

nn_fit <- train(y ~ ., data = train_df, method = 'nnet', maxit = 1000, linout = 0, 
                preProcess = c('center', 'scale'), trControl = numFolds,
                tuneGrid=expand.grid(size=c(9,11,13), decay=c(0.1,0.2)))

## compute logistic loss for binary outcome
#LogLossBinary = function(actual, predicted, eps = 1e-15) {
#  predicted = pmin(pmax(predicted, eps), 1-eps)
#  - (sum(actual * log(predicted) + (1 - actual) * log(1 - predicted))) / length(actual)
#}
## test the sampled data

#results <- predict(nn_fit, newdata=data.frame(test.x))
#LogLossBinary(y_train[-indexes1],results)
pred_df = data.frame(pred_matrix)
col_names(pred_df) = names
## use the optimal model to predict outcome on test datasets
results1 <- predict(nn_fit, newdata=pred_df)

## seperate the private and public datasets
predict_pr = results1[1:60000]
predict_pub = results1[-1:-60000]

## combine the id with no-show probability
prediction_pr1 = data.frame(cbind(pred_pr$ID,predict_pr))
prediction_pub1 = data.frame(cbind(pred_pub$ID,predict_pub))

#write_csv(prediction_pr1,'private.csv',col_names = F)
#write_csv(prediction_pub1,'public.csv',col_names = F)

```
### After parameter tuning, we got size = 12, decay = 0.1
The logistic loss of sample test data is around 0.598, which is good. Tuning the parameters however, takes almost 2 hours. Considering the time of training model, nnet might not be a good choice.

#### Check model performance on test data
```{r}
library(nnet)

## build nn
model_nnet = nnet(y~.,data = train_df,preProcess = c('center', 'scale'), size = 12, rang = 0.1, decay = 0.1, maxit = 1000)

pred_scaled_pr = data.frame(pred_matrix[1:60000,])
pred_scaled_pub = data.frame(pred_matrix[-1:-60000,])

prediction_pr = data.frame(cbind(pred_pr$ID,predict(model_nnet,pred_scaled_pr)))
prediction_pub = data.frame(cbind(pred_pub$ID,predict(model_nnet,pred_scaled_pub)))

#write_csv(prediction_pr,'private.csv',col_names = F)
#write_csv(prediction_pub,'public.csv',col_names = F)

```

The forth predictive model used is K-Nearest Neighbors algorithm which is a non-parametric method used for classification and regression. We first set sqrt(number of observations) as k-fold and then we used cross validation to select k. Finally, we find that the k with the highest accuracy and comparatively low log loss is 346. However, though the accuracy of KNN is as high as 0.686, the log loss is also very high.
```{r}
#KNN
library(KODAMA)
library(dplyr)
library(class)
library(data.table)

train_total <- fread("Predict_NoShow_Train.csv", header = TRUE)
train_total <- train_total[,-4] # delete first date column
train_total <- train_total[,-4] # delete second date column
id <- train_total[,1] # delete ID column
train_total <- train_total[,-1] # save ID column for further use 
train_total$Gender<-as.numeric(factor(train_total$Gender))
train_total$DayOfTheWeek<-as.numeric(factor(train_total$DayOfTheWeek))
train_total$Status<-as.numeric(factor(train_total$Status,levels=c("Show-Up","No-Show")))

test <- fread("Predict_NoShow_PrivateTest_WithoutLabels.csv", header = TRUE)
test <- test[,-4] # delete first date column
test <- test[,-4] # delete second date column
id1 <- test[,1] # delete ID column
test <- test[,-1] # save ID column for further use 
test$Gender<-as.numeric(factor(test$Gender))
test$DayOfTheWeek<-as.numeric(factor(test$DayOfTheWeek))

test2 <- fread("Predict_NoShow_PublicTest_WithoutLabels.csv", header = TRUE)
test2 <- test2[,-4] # delete first date column
test2 <- test2[,-4] # delete second date column
id2 <- test2[,1] # delete ID column
test2 <- test2[,-1] # save ID column for further use 
test2$Gender<-as.numeric(factor(test2$Gender))
test2$DayOfTheWeek<-as.numeric(factor(test2$DayOfTheWeek))

# train data 2/3
#train <- train_total[1:120000]
# test data 1/3
#test <- train_total[120001:180000]

normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x))) }

#normalize covariates
train_total[,1]<-normalize(train_total[,1])
train_total[,3]<-normalize(train_total[,3])
train.x<-as.matrix(train_total[,1:12])

test[,1]<-normalize(test[,1])
test[,3]<-normalize(test[,3])
test.x<-as.matrix(test)

test2[,1]<-normalize(test2[,1])
test2[,3]<-normalize(test2[,3])
test.2<-as.matrix(test2)

#response variable from the train data
response<-as.matrix(train_total[,13])

#use KNN to predict private test
pred<-knn(train = train.x,test = test.x,cl=response,k=424,prob = T)
a<-cbind(id1,attributes(pred)$prob,pred)
a$pred<-factor(a$pred,levels = c(1,2),labels = c(0,1))

#use KNN to predict public test
pred2<-knn(train = train.x,test = test.2,cl=response,k=424,prob = T)
b<-cbind(id2,attributes(pred2)$prob,pred=pred2)
b$pred<-factor(b$pred,levels = c(1,2),labels = c(0,1))

#write.table(a,"private.csv",row.names = F,col.names = F,sep = ",")
#write.table(b,"public.csv",row.names = F,col.names = F, sep = ",")
```

### Parameter Tuning
```{r}
# transfer data into model matrix
data.train <- model.matrix(~., data = data.train)[,-1]


# Change column name
colnames(data.train)[18] <- "Status"
# Label no-show as 1
data.train[,18]<-abs(data.train[,18]-1)

x <- subset(data.train, select = -c(Status))
y <- subset(data.train, select = Status)

system.time(grid <- expand.grid(nrounds   = 250, #c(250,300)
                    max_depth = c(6,7,8),
                    eta       = 0.01,
                    gamma     = c(4,5),
                    colsample_bytree = 0.5,
                    min_child_weight = 1,
                    subsample = 0.5))

# 10 fold Cross-validation
system.time(ctrl <- trainControl(method = "cv",
                     number = 5,
                     summaryFunction = mnLogLoss, 
                     classProbs      = TRUE,
                     allowParallel   = TRUE))

set.seed(1234) # Set the seed to create reproducible train and test sets.
system.time(xgbTune <- caret::train(x = x,
                        y=as.factor(make.names(y)),
                method    = "xgbTree",
                tuneGrid  = grid,
                verbose=T,
                metric="logLoss",
                trControl = ctrl))
```

### After parameter tuning, we got nrounds=250, eta=0.01, max_depth=7, gamma =4
#### Check model performance on test data
```{r}
set.seed(111)
train_rows <- sample(1:nrow(train), (2/3)*nrow(train))
x.train <- x[train_rows,]
x.test <- x[-train_rows,]

y.train <- y[train_rows,]
y.test <- y[-train_rows,]

dtrain <- xgb.DMatrix(data = x.train, label = y.train)
dtest <- xgb.DMatrix(data = x.test, label = y.test)

xgb_params <- list(
  colsample_bytree = 0.5, #It control the number of features (variables) supplied to a tree
  subsample = 0.5, #It controls the number of samples (observations) supplied to a tree
  booster = "gbtree",
  max_depth = 7, #It controls the depth of the tree
  eta = 0.01, #shrinkage rate to control overfitting through conservative approach
  gamma = 4,
  min_child_weight=1,
  objective = "binary:logistic"
  )

xgb.t <- xgb.train (params = xgb_params, data = dtrain, nrounds = 250, watchlist = list(val=dtest,train=dtrain), print_every_n = 50, maximize = F , eval_metric = "logloss")

### Check auc and logloss on test data
pred.t <- predict(xgb.t,x.test)
# Test auc
roc_obj <- roc(y.test,pred.t)
#auc(roc_obj)

# Log loss
LogLoss(pred.t, y.test)
```

#### Training the model on the whole dataset
```{r}
# Use logloss as evaluation metric
xgb <- xgboost(data = x, label = y, nrounds = 250,eta=0.01, gamma=4, max_depth=7, min_child_weight=1, subsample=0.5, colsample_bytree=0.5,objective = "binary:logistic", eval_metric = "logloss",print_every_n = 50)

```


```{r}

Private_test <- fread("/Users/weiqipan/Desktop/Predict_NoShow_PrivateTest_WithoutLabels.csv")
Public_test <- fread("/Users/weiqipan/Desktop/Predict_NoShow_PublicTest_WithoutLabels.csv")
#Private
Private_test$DateOfAppointment <- as.Date(Private_test$DateOfAppointment)
Private_test$month <- format(Private_test$DateOfAppointment,"%m")

Private.x.test <- subset(Private_test, select = -c(ID,DateAppointmentWasMade,DateOfAppointment))
Private.x.test <- model.matrix(~., data = Private.x.test)[,-1]

Private.pred.xgb <- predict(xgb,Private.x.test)
Private.pred.xgb <- cbind(Private_test$ID,Private.pred.xgb )
write.table(Private.pred.xgb, file = "private.csv", sep = ",", col.names=FALSE, row.names = FALSE)


#Public
Public_test$DateOfAppointment <- as.Date(Public_test$DateOfAppointment)
Public_test$month <- format(Public_test$DateOfAppointment,"%m")

Public.x.test <- subset(Public_test, select = -c(ID,DateAppointmentWasMade,DateOfAppointment))
Public.x.test <- model.matrix(~., data = Public.x.test)[,-1]

Public.pred.xgb <- predict(xgb,Public.x.test)
Public.pred.xgb <- cbind(Public_test$ID,Public.pred.xgb )
write.table(Public.pred.xgb, file = "public.csv", sep = ",", col.names=FALSE, row.names = FALSE)

```

###Penalized logistic regression
```{r}
# Create a vector of lambda values
lambda <- 10^seq(10, -2, length = 100)

# Lasso model
x1 <- as.matrix(x)
y1 <- as.factor(y)

a <- rep(0,nrow(Private.x.test))
month02 = a
month03 = a
month04 = a
month05 = a
month06 = a
month07 = a
month08 = a
month09 = a
month10 = a
month11 = a
month12 = a
DayOfTheWeekSunday = a

lasso.mod <- glmnet(x1, y1,family = "binomial", alpha = 1, lambda = lambda)

cv.out.lasso <- cv.glmnet(x1, y1,family = "binomial", alpha = 1, lambda = lambda)
bestlam.lasso <- cv.out.lasso$lambda.min

Private.l <- cbind(Private.x.test,month02,month03, month04,month05,month06,month07,DayOfTheWeekSunday)
Private.pred.l <- predict(lasso.mod, s = bestlam.lasso, newx = Private.l,type = "response")

Public.l <- cbind(Public.x.test,month02,month03, month08,month09,month10,month11,month12,DayOfTheWeekSunday)
Public.pred.l <- predict(lasso.mod, s = bestlam.lasso, newx = Public.l,type = "response")


# ridge model

ridge.mod <- glmnet(x1, y1,family = "binomial", alpha = 0, lambda = lambda)

cv.out.ridge <- cv.glmnet(x1, y1,family = "binomial", alpha = 0, lambda = lambda)
bestlam.ridge <- cv.out.ridge$lambda.min

#df.Private <- as.data.frame(Private.x.test)

Private.pred.r <- predict(ridge.mod, s = bestlam.ridge, newx = Private.l,type = "response")
Private.pred.r <- cbind(Private_test$ID,Private.pred.r )
#write.table(Private.pred.r, file = "private.csv", sep = ",", col.names=FALSE, row.names = FALSE)

Public.pred.r <- predict(ridge.mod, s = bestlam.ridge, newx = Public.l,type = "response")
Public.pred.r <- cbind(Public_test$ID,Public.pred.r )
#write.table(Public.pred.r, file = "public.csv", sep = ",", col.names=FALSE, row.names = FALSE)

```

###Conclusion: Perform poorly when we submit the prediction on leaderboards

