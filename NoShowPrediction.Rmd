---
title: "prediction"
author: "Annie Yang"
date: "5/7/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This project aims to predict and improve the missed opportunity rate (no-show rate) for medical centers. The data is stored in the dataset Predict_NoShow_Train.csv with 180,000 observations and 16 predictors.

```{r}
# import libraries
library(xgboost)
library(data.table)
library(Matrix)
library(pROC)
library(dplyr)
library(purrr)
library(tidyverse)
library(lubridate)
library(Matrix)
library(MLmetrics)
library(caret)
library(glmnet)
```

First, we imported the original dataset and cleaned the missing data. Since no missing value was found, all the 180,000 observations are included in the following analysis and prediction. 
```{r}
# Import data
train <- fread("/Users/weiqipan/Desktop/Predict_NoShow_Train.csv")

# Check missing data

train%>%
  map_df(function(col) sum(is.na(col)))%>%
  gather()%>%
  summarise(count_of_nas = sum(value,na.rm=T))

### there is no missing data in the dataset
```

Then, some variables are transformed or removed from the training set for better prediction. We extracted the Month and Year from the variable Date of Appointment and made Month as separate predictive variables. When making analysis, we chose to focus more on the data of appointment because the date of appointment could be correlated with the missing opportunity rate. People could chose to not show-up due to the weather or time factors. 
```{r}
# Manipulate date
#train$DateAppointmentWasMade <- as.Date(train$DateAppointmentWasMade)
train$DateOfAppointment <- as.Date(train$DateOfAppointment)
# extract month and year information
train$month <- format(train$DateOfAppointment,"%m")
train$year <- format(train$DateOfAppointment,"%y")
```

To make further and more complete analysis, we made barplots to interpret the ratio of No Show-up rate in different months and years. The ratio of No-Show rate varies across the year, from the lowerst 0.282 in January to highest 0.32 in December. Because of the fluctuation, month is considered as a predictive variable in the model.

```{r}
# appointment data
# frequency distribution for month
month_s <- train$month[which(train$Status == "Show-Up")]
month_ns <- train$month[which(train$Status == "No-Show")]

month_s_freq <- data.frame(table(month_s))
month_ns_freq <- data.frame(table(month_ns))


Month <- c("01", "02","03","04","05","06","07","08","09","10","11","12")
month_s_freq <- c(month_s_freq$Freq)
month_ns_freq <- c(month_ns_freq$Freq)
text <- month_ns_freq/(month_s_freq+month_ns_freq)
data <- data.frame(Month, month_s_freq, month_ns_freq,text)

p1 <- plot_ly(data, x = ~Month, y = ~month_s_freq, type = 'bar', name = 'Show-Up',text=text) %>%
  add_trace(y = ~month_ns_freq, name = 'No-Show') %>%
  layout(yaxis = list(title = 'Count'), barmode = 'stack')
p1
```

Then we chose to visualize the rate of No-Show rate for year as well. From the below graph, we can tell that the No-Show rate for year 2014 and 2015 are almost the same at the level 0.30, which means that different year would not affect the No-Show rate. So, year variable is excluded from the predictive model.
```{r}
# appointment data
# frequency distribution for year
year_s <- train$year[which(train$Status == "Show-Up")]
year_ns <- train$year[which(train$Status == "No-Show")]

year_s_freq <- data.frame(table(year_s))
year_ns_freq <- data.frame(table(year_ns))

Year <- c("2014","2015")
year_s_freq <- c(year_s_freq$Freq)
year_ns_freq <- c(year_ns_freq$Freq)
text <- year_ns_freq/(year_s_freq+year_ns_freq)
data <- data.frame(Year, year_s_freq, year_ns_freq,text)

p2<- plot_ly(data, x = ~Year, y = ~year_s_freq, type = 'bar', name = 'Show-Up',text=text) %>%
  add_trace(y = ~year_ns_freq, name = 'No-Show') %>%
  layout(yaxis = list(title = 'Count'), barmode = 'stack')
p2
```

The data after cleaning has 180,000 observations and 14 variables and will be used as the training data later. The result No-Show is set to be 1, and Show-up as 0. Then we fit logistic regression, Penalized logistic regression, and XGBoost to train the dataset and make comparisons with the test data.

```{r}
# Remove unnecessary variable
data.train <- subset(train, select = -c(ID,DateAppointmentWasMade,DateOfAppointment,year))
```

The first method we tried is logistic regression.After factorization, the result is 1 when the person did not show and is 0 when the person showed up, which is a binary result. So we chose to fit a logistic regression, which could be used to model the probability of not showing up. In this model, month is factorized, and a logistic regression is fit on the 14 variables.
```{r}
可以在这加入内容





```

The third predictive model used is K-Nearest Neighbors algorithm which is a non-parametric method used for classification and regression. We first set sqrt(number of observations) as k-fold and then we used cross validation to select k. Finally, we find that the k with the highest accuracy and comparatively low log loss is 346. However, though the accuracy of KNN is as high as 0.686, the log loss is also very high.
```{r}
#KNN
library(KODAMA)
library(dplyr)
library(class)
library(data.table)

train_total <- fread("Predict_NoShow_Train.csv", header = TRUE)
train_total <- train_total[,-4] # delete first date column
train_total <- train_total[,-4] # delete second date column
id <- train_total[,1] # delete ID column
train_total <- train_total[,-1] # save ID column for further use 
train_total$Gender<-as.numeric(factor(train_total$Gender))
train_total$DayOfTheWeek<-as.numeric(factor(train_total$DayOfTheWeek))
train_total$Status<-as.numeric(factor(train_total$Status,levels=c("Show-Up","No-Show")))

test <- fread("Predict_NoShow_PrivateTest_WithoutLabels.csv", header = TRUE)
test <- test[,-4] # delete first date column
test <- test[,-4] # delete second date column
id1 <- test[,1] # delete ID column
test <- test[,-1] # save ID column for further use 
test$Gender<-as.numeric(factor(test$Gender))
test$DayOfTheWeek<-as.numeric(factor(test$DayOfTheWeek))

test2 <- fread("Predict_NoShow_PublicTest_WithoutLabels.csv", header = TRUE)
test2 <- test2[,-4] # delete first date column
test2 <- test2[,-4] # delete second date column
id2 <- test2[,1] # delete ID column
test2 <- test2[,-1] # save ID column for further use 
test2$Gender<-as.numeric(factor(test2$Gender))
test2$DayOfTheWeek<-as.numeric(factor(test2$DayOfTheWeek))

# train data 2/3
#train <- train_total[1:120000]
# test data 1/3
#test <- train_total[120001:180000]

normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x))) }

#normalize covariates
train_total[,1]<-normalize(train_total[,1])
train_total[,3]<-normalize(train_total[,3])
train.x<-as.matrix(train_total[,1:12])

test[,1]<-normalize(test[,1])
test[,3]<-normalize(test[,3])
test.x<-as.matrix(test)

test2[,1]<-normalize(test2[,1])
test2[,3]<-normalize(test2[,3])
test.2<-as.matrix(test2)

#response variable from the train data
response<-as.matrix(train_total[,13])

#use KNN to predict private test
pred<-knn(train = train.x,test = test.x,cl=response,k=424,prob = T)
a<-cbind(id1,attributes(pred)$prob,pred)
a$pred<-factor(a$pred,levels = c(1,2),labels = c(0,1))

#use KNN to predict public test
pred2<-knn(train = train.x,test = test.2,cl=response,k=424,prob = T)
b<-cbind(id2,attributes(pred2)$prob,pred=pred2)
b$pred<-factor(b$pred,levels = c(1,2),labels = c(0,1))

#write.table(a,"private.csv",row.names = F,col.names = F,sep = ",")
#write.table(b,"public.csv",row.names = F,col.names = F, sep = ",")
```

### Parameter Tuning
```{r}
# transfer data into model matrix
data.train <- model.matrix(~., data = data.train)[,-1]


# Change column name
colnames(data.train)[18] <- "Status"
# Label no-show as 1
data.train[,18]<-abs(data.train[,18]-1)

x <- subset(data.train, select = -c(Status))
y <- subset(data.train, select = Status)

system.time(grid <- expand.grid(nrounds   = 250, #c(250,300)
                    max_depth = c(6,7,8),
                    eta       = 0.01,
                    gamma     = c(4,5),
                    colsample_bytree = 0.5,
                    min_child_weight = 1,
                    subsample = 0.5))

# 10 fold Cross-validation
system.time(ctrl <- trainControl(method = "cv",
                     number = 5,
                     summaryFunction = mnLogLoss, 
                     classProbs      = TRUE,
                     allowParallel   = TRUE))

set.seed(1234) # Set the seed to create reproducible train and test sets.
system.time(xgbTune <- caret::train(x = x,
                        y=as.factor(make.names(y)),
                method    = "xgbTree",
                tuneGrid  = grid,
                verbose=T,
                metric="logLoss",
                trControl = ctrl))
```

### After parameter tuning, we got nrounds=250, eta=0.01, max_depth=7, gamma =4
#### Check model performance on test data
```{r}
set.seed(111)
train_rows <- sample(1:nrow(train), (2/3)*nrow(train))
x.train <- x[train_rows,]
x.test <- x[-train_rows,]

y.train <- y[train_rows,]
y.test <- y[-train_rows,]

dtrain <- xgb.DMatrix(data = x.train, label = y.train)
dtest <- xgb.DMatrix(data = x.test, label = y.test)

xgb_params <- list(
  colsample_bytree = 0.5, #It control the number of features (variables) supplied to a tree
  subsample = 0.5, #It controls the number of samples (observations) supplied to a tree
  booster = "gbtree",
  max_depth = 7, #It controls the depth of the tree
  eta = 0.01, #shrinkage rate to control overfitting through conservative approach
  gamma = 4,
  min_child_weight=1,
  objective = "binary:logistic"
  )

xgb.t <- xgb.train (params = xgb_params, data = dtrain, nrounds = 250, watchlist = list(val=dtest,train=dtrain), print_every_n = 50, maximize = F , eval_metric = "logloss")

### Check auc and logloss on test data
pred.t <- predict(xgb.t,x.test)
# Test auc
roc_obj <- roc(y.test,pred.t)
#auc(roc_obj)

# Log loss
LogLoss(pred.t, y.test)
```

#### Training the model on the whole dataset
```{r}
# Use logloss as evaluation metric
xgb <- xgboost(data = x, label = y, nrounds = 250,eta=0.01, gamma=4, max_depth=7, min_child_weight=1, subsample=0.5, colsample_bytree=0.5,objective = "binary:logistic", eval_metric = "logloss",print_every_n = 50)

```


```{r}

Private_test <- fread("/Users/weiqipan/Desktop/Predict_NoShow_PrivateTest_WithoutLabels.csv")
Public_test <- fread("/Users/weiqipan/Desktop/Predict_NoShow_PublicTest_WithoutLabels.csv")
#Private
Private_test$DateOfAppointment <- as.Date(Private_test$DateOfAppointment)
Private_test$month <- format(Private_test$DateOfAppointment,"%m")

Private.x.test <- subset(Private_test, select = -c(ID,DateAppointmentWasMade,DateOfAppointment))
Private.x.test <- model.matrix(~., data = Private.x.test)[,-1]

Private.pred.xgb <- predict(xgb,Private.x.test)
Private.pred.xgb <- cbind(Private_test$ID,Private.pred.xgb )
write.table(Private.pred.xgb, file = "private.csv", sep = ",", col.names=FALSE, row.names = FALSE)


#Public
Public_test$DateOfAppointment <- as.Date(Public_test$DateOfAppointment)
Public_test$month <- format(Public_test$DateOfAppointment,"%m")

Public.x.test <- subset(Public_test, select = -c(ID,DateAppointmentWasMade,DateOfAppointment))
Public.x.test <- model.matrix(~., data = Public.x.test)[,-1]

Public.pred.xgb <- predict(xgb,Public.x.test)
Public.pred.xgb <- cbind(Public_test$ID,Public.pred.xgb )
write.table(Public.pred.xgb, file = "public.csv", sep = ",", col.names=FALSE, row.names = FALSE)

```

###Penalized logistic regression
```{r}
# Create a vector of lambda values
lambda <- 10^seq(10, -2, length = 100)

# Lasso model
x1 <- as.matrix(x)
y1 <- as.factor(y)

a <- rep(0,nrow(Private.x.test))
month02 = a
month03 = a
month04 = a
month05 = a
month06 = a
month07 = a
month08 = a
month09 = a
month10 = a
month11 = a
month12 = a
DayOfTheWeekSunday = a

lasso.mod <- glmnet(x1, y1,family = "binomial", alpha = 1, lambda = lambda)

cv.out.lasso <- cv.glmnet(x1, y1,family = "binomial", alpha = 1, lambda = lambda)
bestlam.lasso <- cv.out.lasso$lambda.min

Private.l <- cbind(Private.x.test,month02,month03, month04,month05,month06,month07,DayOfTheWeekSunday)
Private.pred.l <- predict(lasso.mod, s = bestlam.lasso, newx = Private.l,type = "response")

Public.l <- cbind(Public.x.test,month02,month03, month08,month09,month10,month11,month12,DayOfTheWeekSunday)
Public.pred.l <- predict(lasso.mod, s = bestlam.lasso, newx = Public.l,type = "response")


# ridge model

ridge.mod <- glmnet(x1, y1,family = "binomial", alpha = 0, lambda = lambda)

cv.out.ridge <- cv.glmnet(x1, y1,family = "binomial", alpha = 0, lambda = lambda)
bestlam.ridge <- cv.out.ridge$lambda.min

#df.Private <- as.data.frame(Private.x.test)

Private.pred.r <- predict(ridge.mod, s = bestlam.ridge, newx = Private.l,type = "response")
Private.pred.r <- cbind(Private_test$ID,Private.pred.r )
#write.table(Private.pred.r, file = "private.csv", sep = ",", col.names=FALSE, row.names = FALSE)

Public.pred.r <- predict(ridge.mod, s = bestlam.ridge, newx = Public.l,type = "response")
Public.pred.r <- cbind(Public_test$ID,Public.pred.r )
#write.table(Public.pred.r, file = "public.csv", sep = ",", col.names=FALSE, row.names = FALSE)

```

###Conclusion: Perform poorly when we submit the prediction on leaderboards

